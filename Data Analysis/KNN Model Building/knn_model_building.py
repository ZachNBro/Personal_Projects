# -*- coding: utf-8 -*-
"""KNN_model_building.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dIHW1ojSgGbfQ88l10qNvcl53ChW8p7o
"""

"""
    File name: knn_model_building.py
    Author: Zachary Brown
    Python Version 3.10.12
    Description: This program analyzes customer churn data from
    a telephone company and uses the KNN algorithm to help find 
    out what factors cause customer churn.
    
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from imblearn.over_sampling import RandomOverSampler

# Load the dataset
dataset = pd.read_csv('/content/sample_data/Churn.csv')

# Show basic dataset info
print("Our dataset has", dataset.shape[0], "rows and", dataset.shape[1], "columns.")
print("Let's see how many customers churned and how many didn't:")
print(dataset['Churn'].value_counts())
print("Are there any missing values in our dataset?", dataset.isnull().any().any())
print("Here's the type of data we have:")
print(dataset.dtypes)

# Convert 'Churn' from text to numbers
le = LabelEncoder()
dataset['Churn'] = le.fit_transform(dataset['Churn'])

# Scale our numerical data to be on the same scale
scaler = MinMaxScaler()
numerical_features = ['Account length', 'Area code', 'Total day minutes',
                      'Total day calls', 'Total day charge', 'Total eve minutes',
                      'Total eve calls', 'Total eve charge', 'Total night minutes',
                      'Total night calls', 'Total night charge', 'Total intl minutes',
                      'Total intl calls', 'Total intl charge', 'Customer service calls']
dataset[numerical_features] = scaler.fit_transform(dataset[numerical_features])
print(dataset.head(5))

# Here, we look at correlations between features
plt.figure(figsize=(10, 8))
sns.heatmap(dataset.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

# We'll also look at some box plots to see how features vary with churn
for feature in numerical_features:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x='Churn', y=feature, data=dataset)
    plt.title(f'Box Plot of {feature} by Churn')
    plt.xlabel('Churn')
    plt.ylabel(feature)
    plt.show()

# A parallel coordinates plot to see how our selected features relate to churn (first iteration)
selected_features = ['Customer service calls', 'Total intl charge', 'Total intl calls',
                     'Total intl minutes', 'Total night charge', 'Total night minutes',
                     'Total eve charge', 'Total eve minutes', 'Total day charge', 'Churn']
selected_data = dataset[selected_features]
plt.figure(figsize=(15, 8))
pd.plotting.parallel_coordinates(selected_data, 'Churn', colormap='viridis', alpha=0.5)
plt.xticks(rotation=45)
plt.title('Parallel Coordinates Plot (First Iteration)')
plt.tight_layout()
plt.show()

# Second iteration of the parallel coordinates plot
selected_features_2 = ['Customer service calls', 'Total intl minutes', 'Total night minutes',
                     'Total eve minutes', 'Total day minutes',  'Churn']
selected_data_2 = dataset[selected_features_2]
plt.figure(figsize=(15, 8))
pd.plotting.parallel_coordinates(selected_data_2, 'Churn', colormap='viridis', alpha=0.5)
plt.xticks(rotation=45)
plt.title('Parallel Coordinates Plot (Second Iteration)')
plt.tight_layout()
plt.show()

# Third iteration of the parallel coordinates plot
selected_features_3 = ['Total eve minutes', 'Customer service calls', 'Total day minutes',
                       'Churn']

selected_data_3 = dataset[selected_features_3]
plt.figure(figsize=(15, 8))
pd.plotting.parallel_coordinates(selected_data_3, 'Churn', colormap='viridis', alpha=0.5)
plt.xticks(rotation=45)
plt.title('Parallel Coordinates Plot (Final 3 Features)')
plt.tight_layout()
plt.show()

# Split our data into training and testing sets
final_features = ['Customer service calls', 'Total day minutes', 'Total eve minutes']

X = dataset[final_features].values
y = dataset['Churn'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Let's build a model using the K Nearest Neighbors algorithm
classifier = KNeighborsClassifier(n_neighbors=13)
classifier.fit(X_train, y_train)

# Now, let's see how our model performs
y_pred = classifier.predict(X_test)
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

# We'll also do some cross-validation to find the best number of neighbors (K)
k_list = list(range(1, 50, 2))
cv_scores = []
for k in k_list:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')
    cv_scores.append(scores.mean())

# Let's plot the misclassification error to see which K value works best
plt.figure(figsize=(15, 10))
plt.plot(k_list, [1 - x for x in cv_scores])
plt.title('Misclassification Error vs. Number of Neighbors (K)')
plt.xlabel('Number of Neighbors (K)')
plt.ylabel('Misclassification Error')
plt.show()

# Finding the best value of K
optimal_k = k_list[np.argmin([1 - x for x in cv_scores])]
print("The optimal number of neighbors is:", optimal_k)
